{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81fa18-1352-470b-ba12-0dcb2fb32316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34b43-e119-4a9d-8f82-2490b72ac55b",
   "metadata": {},
   "source": [
    "Implement 1st ML Classifier: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ea4ee-8517-4616-b65c-d38d5894b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "internet_data = pd.read_csv('preprocessed_internet_data.csv')\n",
    "\n",
    "internet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ca058-3825-4d5c-b4c9-31dd21422d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e07c5-f41a-421f-b4c9-850eda7815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = internet_data.drop('Action', axis=1)  \n",
    "y = internet_data['Action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cdfc9-8148-4610-8c1e-a191c8709e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_ports = internet_data['NAT Source Port']\n",
    "nat_ports\n",
    "\n",
    "target_variable = y \n",
    "\n",
    "relationship_df = pd.DataFrame({\n",
    "    'nat source port': nat_ports,\n",
    "    'Target': target_variable\n",
    "})\n",
    "\n",
    "relationship_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808b280-fb6e-49e2-8aff-1e5722e20f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a1609-66c4-4d55-a8da-1bb335356d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54591ce7-7775-4632-baac-68c001b81ae7",
   "metadata": {},
   "source": [
    "1. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cddc73-5b83-4eda-b38b-782449909804",
   "metadata": {},
   "source": [
    "Let's first assess feature importance so we can figure out which features contribute the most to our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e331033-3110-4d6b-9971-51b8ee2f86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=2)\n",
    "\n",
    "# Stratified KFold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "cv_results = cross_validate(model, X, y, cv=skf, return_estimator=True)\n",
    "\n",
    "for estimator in cv_results['estimator']:\n",
    "    feature_importances.append(estimator.feature_importances_)\n",
    "\n",
    "# Calculate and display mean metrics\n",
    "mean_metrics = {key: np.mean(values) for key, values in cv_results.items() if key.startswith('test_')}\n",
    "print(\"Average Metrics across folds:\")\n",
    "for metric, value in mean_metrics.items():\n",
    "    print(f\"  {metric.replace('test_', '').capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Convert the list of feature importances to a DataFrame for easier interpretation\n",
    "feature_importances = np.array(feature_importances)\n",
    "\n",
    "# Average the feature importances across folds\n",
    "mean_importances = feature_importances.mean(axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding importances\n",
    "feature_names = X.columns  \n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': mean_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances in Random Forest (Average across folds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dde988-5db1-41f6-9fa7-948e925e79e4",
   "metadata": {},
   "source": [
    "Interpreting the feature importance graph, we see that the two most important features are NAT Source Port and Elapsed Time. Let's use random forest with a depth of 2 and a singular decision tree with a depth of 2 to test the accuracy of our model using these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13c7f8-f875-4c21-9751-35f9e92d5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=42)\n",
    "\n",
    "# Stratified KFold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Store metrics and confusion matrices for each fold\n",
    "tree_accuracies, rf_accuracies = [], []\n",
    "tree_conf_matrices, rf_conf_matrices = [], []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train and predict with Decision Tree\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    tree_preds = tree_model.predict(X_test)\n",
    "    tree_accuracies.append(accuracy_score(y_test, tree_preds))\n",
    "    tree_conf_matrices.append(confusion_matrix(y_test, tree_preds))\n",
    "\n",
    "    # Train and predict with Random Forest\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_accuracies.append(accuracy_score(y_test, rf_preds))\n",
    "    rf_conf_matrices.append(confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "# Average accuracies\n",
    "avg_tree_accuracy = np.mean(tree_accuracies)\n",
    "avg_rf_accuracy = np.mean(rf_accuracies)\n",
    "\n",
    "print(f\"Average Accuracy (Decision Tree, max_depth=2): {avg_tree_accuracy:.4f}\")\n",
    "print(f\"Average Accuracy (Random Forest, n_estimators=100, max_depth=2): {avg_rf_accuracy:.4f}\")\n",
    "\n",
    "# Visualize confusion matrices for the last fold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ConfusionMatrixDisplay(tree_conf_matrices[-1], display_labels=tree_model.classes_).plot(ax=axes[0], cmap=\"Blues\")\n",
    "axes[0].set_title(\"Decision Tree Confusion Matrix (Last Fold)\")\n",
    "\n",
    "ConfusionMatrixDisplay(rf_conf_matrices[-1], display_labels=rf_model.classes_).plot(ax=axes[1], cmap=\"Blues\")\n",
    "axes[1].set_title(\"Random Forest Confusion Matrix (Last Fold)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed47db-757d-4aa2-8605-327d1766df37",
   "metadata": {},
   "source": [
    "We see that there is not much of a difference between the accuracy of the single decision tree and random forest. We can consider using one decision tree for this data since it computationally inexpensive, especially with a depth of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441de81-8368-4e6b-8b04-5dfc673d143f",
   "metadata": {},
   "source": [
    "2. K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4cf55-94ec-4ad8-90f4-bbc010e025ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Range of k values to test\n",
    "k_range = range(1, 21)  \n",
    "\n",
    "# Store the accuracy for each k\n",
    "knn_accuracies = []\n",
    "\n",
    "# Stratified K-Fold Cross-Validation for each k value\n",
    "for k in k_range:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    fold_accuracies = [] \n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        knn_preds = knn_model.predict(X_test)\n",
    "\n",
    "        fold_accuracies.append(accuracy_score(y_test, knn_preds))\n",
    "\n",
    "    # Average accuracy for this value of k\n",
    "    knn_accuracies.append(np.mean(fold_accuracies))\n",
    "\n",
    "# Plot the elbow curve to find the best value for k\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, knn_accuracies, marker='o', color='b', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.title('KNN Elbow Plot (Accuracy vs. k)', fontsize=14)\n",
    "plt.xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "plt.ylabel('Average Accuracy', fontsize=12)\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d24db8-a92d-42e5-81f5-fc6ea74415d0",
   "metadata": {},
   "source": [
    "The highest accuracy for k looks to be 7, although all the values for k seem to be above 98%. We will stick with k = 7 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd40829-2139-449f-af42-9b51536605db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KNN model with 7 neighbors\n",
    "k_neighbors = 7\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "knn_accuracies = []\n",
    "knn_conf_matrices = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    knn_preds = knn_model.predict(X_test)\n",
    "\n",
    "    knn_accuracies.append(accuracy_score(y_test, knn_preds))\n",
    "    knn_conf_matrices.append(confusion_matrix(y_test, knn_preds))\n",
    "\n",
    "# Average accuracy\n",
    "avg_knn_accuracy = np.mean(knn_accuracies)\n",
    "\n",
    "print(f\"Average Accuracy (KNN, k={k_neighbors}): {avg_knn_accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(knn_conf_matrices[-1], display_labels=knn_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(f\"KNN Confusion Matrix (Last Fold, k={k_neighbors})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e4381",
   "metadata": {},
   "source": [
    "3.  Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "#print(X_train, y_train)\n",
    "\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f'SVM accuracy score: {acc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c2366",
   "metadata": {},
   "source": [
    "Because we get a high accuracy, we can verify this by doing k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified KFold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "svm_accuracies = []\n",
    "svm_conf_matrices = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    svm_preds = svm_model.predict(X_test)\n",
    "\n",
    "    svm_accuracies.append(accuracy_score(y_test, svm_preds))\n",
    "    svm_conf_matrices.append(confusion_matrix(y_test, svm_preds))\n",
    "\n",
    "# Average accuracy\n",
    "avg_svm_accuracy = np.mean(svm_accuracies)\n",
    "\n",
    "print(f\"SVM Average Accuracy: {avg_svm_accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(svm_conf_matrices[-1], display_labels=svm_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(f\"SVM Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db013a",
   "metadata": {},
   "source": [
    "Using SVM and what we determined to be the two most important features, we can see that this version model gives up 79% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X2 = X[['NAT Source Port', 'Elapsed Time (sec)']] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "#print(X_train, y_train)\n",
    "\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f'SVM accuracy score: {acc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ff5d6",
   "metadata": {},
   "source": [
    "4. Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "score = linear_model.score(X_test, y_test)\n",
    "print(f'linear model score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d5b3a",
   "metadata": {},
   "source": [
    "We can use K-fold cross validation to analyze our linear model. Here we see it has a high value for R² showing it explains a good amount of the variance in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec97e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "linear_accuracies = []\n",
    "linear_conf_matrices = []\n",
    "linear_rmse = []\n",
    "linear_r2 = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    linear_preds = linear_model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, linear_preds))\n",
    "    r2 = r2_score(y_test, linear_preds)\n",
    "\n",
    "    linear_rmse.append(rmse)\n",
    "    linear_r2.append(r2)\n",
    "\n",
    "avg_rmse = np.mean(linear_rmse)\n",
    "avg_r2 = np.mean(linear_r2)\n",
    "\n",
    "print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "print(f\"Average R²: {avg_r2:.4f}\")\n",
    "\n",
    "# create a plot of the residuals vs predicted values\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(linear_preds, y_test - linear_preds, color='blue')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Reference line at y=0\n",
    "plt.title(\"Residuals vs. Predicted Values\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(X.columns)\n",
    "num_cols = 3\n",
    "num_rows = (num_features + num_cols - 1) // num_cols  # Calculate number of rows to fit all features\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for idx, feature in enumerate(X.columns):\n",
    "    \n",
    "    linear_model.fit(internet_data[[feature]], y)\n",
    "    y_pred = linear_model.predict(internet_data[[feature]])\n",
    "  \n",
    "    axes[idx].scatter(internet_data[feature], y, color='blue', label='Actual Data')\n",
    "    axes[idx].plot(internet_data[feature], y_pred, color='red', label='Regression Line')\n",
    "    \n",
    "    axes[idx].set_title(f\"Linear Regression: Action vs {feature}\")\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel(\"Action\")\n",
    "    axes[idx].legend()\n",
    "\n",
    "for idx in range(num_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5df47",
   "metadata": {},
   "source": [
    "5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# do some standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# reduce the features so we can graph our model\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train the Multinomial Logistic Regression model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# use mesh grid to help plot diecision boundary\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# create the prediction areas\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# create the plot.\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, edgecolors='k', marker='o', cmap=plt.cm.Paired)\n",
    "plt.title('Multinomial Logistic Regression Decision Boundary')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
