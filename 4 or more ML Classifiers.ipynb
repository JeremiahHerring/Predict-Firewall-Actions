{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81fa18-1352-470b-ba12-0dcb2fb32316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34b43-e119-4a9d-8f82-2490b72ac55b",
   "metadata": {},
   "source": [
    "Implement 1st ML Classifier: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ea4ee-8517-4616-b65c-d38d5894b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "internet_data = pd.read_csv('preprocessed_internet_data.csv')\n",
    "\n",
    "internet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ca058-3825-4d5c-b4c9-31dd21422d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e07c5-f41a-421f-b4c9-850eda7815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = internet_data.drop('Action', axis=1)  \n",
    "y = internet_data['Action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cdfc9-8148-4610-8c1e-a191c8709e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_ports = internet_data['NAT Source Port']\n",
    "nat_ports\n",
    "\n",
    "target_variable = y \n",
    "\n",
    "relationship_df = pd.DataFrame({\n",
    "    'nat source port': nat_ports,\n",
    "    'Target': target_variable\n",
    "})\n",
    "\n",
    "relationship_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808b280-fb6e-49e2-8aff-1e5722e20f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a1609-66c4-4d55-a8da-1bb335356d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54591ce7-7775-4632-baac-68c001b81ae7",
   "metadata": {},
   "source": [
    "1. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cddc73-5b83-4eda-b38b-782449909804",
   "metadata": {},
   "source": [
    "Let's first assess feature importance so we can figure out which features contribute the most to our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e331033-3110-4d6b-9971-51b8ee2f86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=2)\n",
    "\n",
    "# Stratified KFold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "cv_results = cross_validate(model, X, y, cv=skf, return_estimator=True)\n",
    "\n",
    "for estimator in cv_results['estimator']:\n",
    "    feature_importances.append(estimator.feature_importances_)\n",
    "\n",
    "# Calculate and display mean metrics\n",
    "mean_metrics = {key: np.mean(values) for key, values in cv_results.items() if key.startswith('test_')}\n",
    "print(\"Average Metrics across folds:\")\n",
    "for metric, value in mean_metrics.items():\n",
    "    print(f\"  {metric.replace('test_', '').capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Convert the list of feature importances to a DataFrame for easier interpretation\n",
    "feature_importances = np.array(feature_importances)\n",
    "\n",
    "# Average the feature importances across folds\n",
    "mean_importances = feature_importances.mean(axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding importances\n",
    "feature_names = X.columns  \n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': mean_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances in Random Forest (Average across folds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dde988-5db1-41f6-9fa7-948e925e79e4",
   "metadata": {},
   "source": [
    "Interpreting the feature importance graph, we see that the two most important features are NAT Source Port and Elapsed Time. Let's use random forest with a depth of 2 and a singular decision tree with a depth of 2 to test the accuracy of our model using these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed47db-757d-4aa2-8605-327d1766df37",
   "metadata": {},
   "source": [
    "We see that there is not much of a difference between the accuracy of the single decision tree and random forest. We can consider using one decision tree for this data since it computationally inexpensive, especially with a depth of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c8bfa-b0d1-499f-845d-1fb7e75037a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics and confusion matrices\n",
    "tree_metrics = []\n",
    "rf_metrics = []\n",
    "tree_conf_matrices = []\n",
    "rf_conf_matrices = []\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train and predict with Decision Tree\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    tree_preds = tree_model.predict(X_test)\n",
    "    tree_probs = tree_model.predict_proba(X_test)[:, 1] if len(tree_model.classes_) == 2 else tree_model.predict_proba(X_test)\n",
    "    tree_conf_matrices.append(confusion_matrix(y_test, tree_preds))\n",
    "\n",
    "    # Compute metrics for Decision Tree\n",
    "    tree_precision = precision_score(y_test, tree_preds, average='weighted')\n",
    "    tree_recall = recall_score(y_test, tree_preds, average='weighted')\n",
    "    tree_f1 = f1_score(y_test, tree_preds, average='weighted')\n",
    "    tree_auc = roc_auc_score(y_test, tree_probs, multi_class='ovr') if len(tree_model.classes_) > 2 else roc_auc_score(y_test, tree_probs)\n",
    "    tree_metrics.append((tree_precision, tree_recall, tree_f1, tree_auc))\n",
    "\n",
    "    # Train and predict with Random Forest\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_probs = rf_model.predict_proba(X_test)[:, 1] if len(rf_model.classes_) == 2 else rf_model.predict_proba(X_test)\n",
    "    rf_conf_matrices.append(confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "    # Compute metrics for Random Forest\n",
    "    rf_precision = precision_score(y_test, rf_preds, average='weighted')\n",
    "    rf_recall = recall_score(y_test, rf_preds, average='weighted')\n",
    "    rf_f1 = f1_score(y_test, rf_preds, average='weighted')\n",
    "    rf_auc = roc_auc_score(y_test, rf_probs, multi_class='ovr') if len(rf_model.classes_) > 2 else roc_auc_score(y_test, rf_probs)\n",
    "    rf_metrics.append((rf_precision, rf_recall, rf_f1, rf_auc))\n",
    "\n",
    "# Calculate averages across folds for both models\n",
    "tree_avg_metrics = pd.DataFrame(tree_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "rf_avg_metrics = pd.DataFrame(rf_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "\n",
    "# Print average metrics for both models\n",
    "print(\"Metrics for Decision Tree:\")\n",
    "print(f\"  Precision: {tree_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {tree_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {tree_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {tree_avg_metrics['AUC']:.4f}\")\n",
    "print()\n",
    "print(\"Metrics for Random Forest:\")\n",
    "print(f\"  Precision: {rf_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {rf_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {rf_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {rf_avg_metrics['AUC']:.4f}\")\n",
    "\n",
    "# Display confusion matrices for the last fold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ConfusionMatrixDisplay(tree_conf_matrices[-1], display_labels=tree_model.classes_).plot(ax=axes[0], cmap=\"Blues\")\n",
    "axes[0].set_title(\"Decision Tree Confusion Matrix (Last Fold)\")\n",
    "\n",
    "ConfusionMatrixDisplay(rf_conf_matrices[-1], display_labels=rf_model.classes_).plot(ax=axes[1], cmap=\"Blues\")\n",
    "axes[1].set_title(\"Random Forest Confusion Matrix (Last Fold)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd010737",
   "metadata": {},
   "source": [
    "Decision Tree/Random Forest Metrics\n",
    "Random Forest achieved outstanding performance metrics with a precision of 0.9916, recall of 0.9923, F1-score of 0.9919, and an AUC of 0.9831. A key advantage of using Random Forest was its ability to assess feature importance, revealing that NAT Source Port and Elapsed Time were the most critical features for making accurate predictions. To prevent overfitting and maintain model simplicity, we limited the Random Forest to a depth of two, which still yielded exceptional metrics. Additionally, adhering to Occam's razor, we tested a single Decision Tree and observed only a slight decrease of about 1\\% in precision, recall, and F1-score. This minimal drop made the Decision Tree the most suitable model for our task despite Random Forest's higher complexity and potential for reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441de81-8368-4e6b-8b04-5dfc673d143f",
   "metadata": {},
   "source": [
    "2. K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4cf55-94ec-4ad8-90f4-bbc010e025ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Range of k values to test\n",
    "k_range = range(1, 21)  \n",
    "\n",
    "# Stratified K-Fold Cross-Validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store the error rate for each k\n",
    "knn_error_rates = []\n",
    "\n",
    "# Stratified K-Fold Cross-Validation for each k value\n",
    "for k in k_range:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    fold_errors = [] \n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        knn_preds = knn_model.predict(X_test)\n",
    "\n",
    "        fold_errors.append(1 - accuracy_score(y_test, knn_preds))\n",
    "\n",
    "    # Average error rate for this value of k\n",
    "    knn_error_rates.append(np.mean(fold_errors))\n",
    "\n",
    "# Plot the elbow curve to find the best value for k\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, knn_error_rates, marker='o', color='r', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.title('KNN Elbow Plot (Error Rate vs. k)', fontsize=14)\n",
    "plt.xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "plt.ylabel('Average Error Rate', fontsize=12)\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"KNN_Elbow_Plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d24db8-a92d-42e5-81f5-fc6ea74415d0",
   "metadata": {},
   "source": [
    "The lowest error rate for k looks to be 3, although all the values for k seem to have low error rates. We will stick with k = 3 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd40829-2139-449f-af42-9b51536605db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KNN model with 3 neighbors\n",
    "k_neighbors = 3\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Lists to store metrics and confusion matrices\n",
    "knn_metrics = []\n",
    "knn_conf_matrices = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the KNN model and make predictions\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    knn_preds = knn_model.predict(X_test)\n",
    "    knn_probs = knn_model.predict_proba(X_test)[:, 1] if len(knn_model.classes_) == 2 else knn_model.predict_proba(X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    knn_precision = precision_score(y_test, knn_preds, average='weighted')\n",
    "    knn_recall = recall_score(y_test, knn_preds, average='weighted')\n",
    "    knn_f1 = f1_score(y_test, knn_preds, average='weighted')\n",
    "    knn_auc = roc_auc_score(y_test, knn_probs, multi_class='ovr') if len(knn_model.classes_) > 2 else roc_auc_score(y_test, knn_probs)\n",
    "    \n",
    "    knn_metrics.append((knn_precision, knn_recall, knn_f1, knn_auc))\n",
    "    knn_conf_matrices.append(confusion_matrix(y_test, knn_preds))\n",
    "\n",
    "# Calculate average metrics\n",
    "knn_avg_metrics = pd.DataFrame(knn_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Average Metrics for KNN (k={k_neighbors}):\")\n",
    "print(f\"  Precision: {knn_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {knn_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {knn_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {knn_avg_metrics['AUC']:.4f}\")\n",
    "\n",
    "# Display the confusion matrix for the last fold\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(knn_conf_matrices[-1], display_labels=knn_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(f\"KNN Confusion Matrix (Last Fold, k={k_neighbors})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e035d",
   "metadata": {},
   "source": [
    "KNN metrics\n",
    "After running KNN with different values of k and plotting the result in an elbow plot, we determined that k = 3 had the lowest error rate out of all k values. This showed very good performance with similar metrics to the best-performing model we tested. The precision achieved by this model was 0.9916, which shows that it has a high-performance ceiling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e4381",
   "metadata": {},
   "source": [
    "3.  Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f'SVM accuracy score: {acc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c2366",
   "metadata": {},
   "source": [
    "Because we get a high accuracy, we can verify this by doing k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize SVM with probability=True for AUC computation\n",
    "svm_model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Lists to store metrics and confusion matrices\n",
    "svm_metrics = []\n",
    "svm_conf_matrices = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the SVM model and make predictions\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    svm_preds = svm_model.predict(X_test)\n",
    "    svm_probs = svm_model.predict_proba(X_test)  \n",
    "\n",
    "    # Compute metrics\n",
    "    svm_precision = precision_score(y_test, svm_preds, average='weighted')\n",
    "    svm_recall = recall_score(y_test, svm_preds, average='weighted')\n",
    "    svm_f1 = f1_score(y_test, svm_preds, average='weighted')\n",
    "    svm_auc = roc_auc_score(\n",
    "        LabelBinarizer().fit_transform(y_test), svm_probs, multi_class='ovr'\n",
    "    )\n",
    "\n",
    "    # Append metrics for this fold\n",
    "    svm_metrics.append((svm_precision, svm_recall, svm_f1, svm_auc))\n",
    "    svm_conf_matrices.append(confusion_matrix(y_test, svm_preds))\n",
    "\n",
    "# Calculate average metrics\n",
    "svm_avg_metrics = pd.DataFrame(svm_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Metrics for SVM:\")\n",
    "print(f\"  Precision: {svm_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {svm_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {svm_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {svm_avg_metrics['AUC']:.4f}\")\n",
    "\n",
    "# Display the confusion matrix for the last fold\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(svm_conf_matrices[-1], display_labels=svm_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(f\"SVM Confusion Matrix (Last Fold)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db013a",
   "metadata": {},
   "source": [
    "Using SVM and what we determined to be the two most important features, we can see that this version model gives up 79% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X2 = X[['NAT Source Port', 'Elapsed Time (sec)']] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "#print(X_train, y_train)\n",
    "\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f'SVM accuracy score: {acc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061427ce",
   "metadata": {},
   "source": [
    "SVM metrics\n",
    "SVM demonstrated strong performance, achieving a precision of 0.9796, recall of 0.9795, F1-score of 0.9793, and an AUC of 0.9443. These high metrics indicate that the SVM effectively balances accuracy and reliability, minimizing both false positives and false negatives. SVM was particularly well-suited for this task due to its ability to handle high-dimensional data and its proficiency in finding optimal hyperplanes that separate different classes, ensuring robust classification of complex network traffic patterns. This made SVM an excellent choice for accurately predicting firewall actions in our numerical data-driven environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5df47",
   "metadata": {},
   "source": [
    "4. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train the Multinomial Logistic Regression model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_test_preds = model.predict(X_test_pca)\n",
    "y_test_probs = model.predict_proba(X_test_pca)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_test_preds, average='weighted')\n",
    "recall = recall_score(y_test, y_test_preds, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_preds, average='weighted')\n",
    "\n",
    "# AUC (for binary and multiclass)\n",
    "if len(set(y_test)) > 2:  # Multiclass case\n",
    "    auc = roc_auc_score(pd.get_dummies(y_test), y_test_probs, multi_class='ovr')\n",
    "else:  # Binary case\n",
    "    auc = roc_auc_score(y_test, y_test_probs[:, 1])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_preds)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Metrics for Logistic Regression with PCA:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "print(f\"  AUC: {auc:.4f}\")\n",
    "\n",
    "logistic_regression_avg_metrics = {'Precision': precision, 'Recall': recall, 'F1-Score': f1, 'AUC': auc}\n",
    "\n",
    "# PCA decision boundary plot\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, edgecolors='k', marker='o', cmap=plt.cm.Paired)\n",
    "plt.title('Multinomial Logistic Regression Decision Boundary with PCA')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Display Confusion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Logistic Regression with PCA Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dae522",
   "metadata": {},
   "source": [
    "Logistic Regression Metrics\n",
    "This model produced slightly worse performance than SVM and KNN, but still ended up with a precision score of 0.9747. To visualize our data, we performed 2D principle component analysis. This dimensionality reduction technique allowed us to observe the spread and clustering of data points, which helped in understanding the underlying structure of the dataset. Despite logistic regression's lower performance, its visualization is helpful in observing trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396ba5b",
   "metadata": {},
   "source": [
    "5. Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160d2db-4574-4e15-8a9d-9139eca4e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Decision Tree', 'Random Forest', 'SVM', 'Logistic Regression']\n",
    "precision = [tree_avg_metrics['Precision'], rf_avg_metrics['Precision'], svm_avg_metrics['Precision'], logistic_regression_avg_metrics['Precision']]\n",
    "recall = [tree_avg_metrics['Recall'], rf_avg_metrics['Recall'], svm_avg_metrics['Recall'], logistic_regression_avg_metrics['Recall']]\n",
    "f1_score = [tree_avg_metrics['F1-Score'], rf_avg_metrics['F1-Score'], svm_avg_metrics['F1-Score'], logistic_regression_avg_metrics['F1-Score']]\n",
    "auc = [tree_avg_metrics['AUC'], rf_avg_metrics['AUC'], svm_avg_metrics['AUC'], logistic_regression_avg_metrics['AUC']]\n",
    "print(models)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1_score)\n",
    "print(auc)\n",
    "\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - 1.5*width, precision, width, label='Precision')\n",
    "rects2 = ax.bar(x - 0.5*width, recall, width, label='Recall')\n",
    "rects3 = ax.bar(x + 0.5*width, f1_score, width, label='F1-Score')\n",
    "rects4 = ax.bar(x + 1.5*width, auc, width, label='AUC')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by model and metric')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim(0.9, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f479fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    'Model': models,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1_score,\n",
    "    'AUC': auc\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Model', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df, annot=True, cmap='coolwarm')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Decision Tree', 'Random Forest', 'SVM', 'Logistic Regression']\n",
    "\n",
    "# Combine all metrics into a single list\n",
    "metrics = [precision, recall, f1_score, auc]\n",
    "\n",
    "# Create a radar chart\n",
    "labels = ['Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "# The radar chart is circular, so we need to \"complete the loop\"\n",
    "angles += angles[:1]\n",
    "\n",
    "min_value = 0.9  # Minimum value for the radar chart\n",
    "max_value = .98  # Maximum value for the radar chart\n",
    "step = 0.05      # Step size for the grid lines\n",
    "\n",
    "# Plot each model's metrics\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "for i, model in enumerate(models):\n",
    "    values = [precision[i], recall[i], f1_score[i], auc[i]]\n",
    "    values += values[:1]  # Complete the loop\n",
    "    ax.plot(angles, values, linewidth=2, label=model)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "    \n",
    "    # Annotate with AUC value\n",
    "    auc_position = (angles[3] + angles[4]) / 2  # Position between the last metric and the first one\n",
    "    ax.text(auc_position, auc[i], f'AUC: {auc[i]:.2f}', horizontalalignment='center', size=12, color='black', weight='semibold')\n",
    "\n",
    "# Draw labels for each metric\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Set the range and add grid lines\n",
    "ax.set_ylim(min_value, max_value)\n",
    "ax.set_yticks(np.arange(min_value, max_value + step, step))\n",
    "ax.yaxis.grid(True, color='gray', linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(True, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Model Comparison on Different Metrics', size=20, color='black', y=1.1)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
