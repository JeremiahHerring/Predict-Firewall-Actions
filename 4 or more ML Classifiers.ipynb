{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81fa18-1352-470b-ba12-0dcb2fb32316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34b43-e119-4a9d-8f82-2490b72ac55b",
   "metadata": {},
   "source": [
    "Implement 1st ML Classifier: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ea4ee-8517-4616-b65c-d38d5894b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "internet_data = pd.read_csv('preprocessed_internet_data.csv')\n",
    "\n",
    "internet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ca058-3825-4d5c-b4c9-31dd21422d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e07c5-f41a-421f-b4c9-850eda7815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = internet_data.drop('Action', axis=1)  \n",
    "y = internet_data['Action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cdfc9-8148-4610-8c1e-a191c8709e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_ports = internet_data['NAT Source Port']\n",
    "nat_ports\n",
    "\n",
    "target_variable = y \n",
    "\n",
    "relationship_df = pd.DataFrame({\n",
    "    'nat source port': nat_ports,\n",
    "    'Target': target_variable\n",
    "})\n",
    "\n",
    "relationship_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808b280-fb6e-49e2-8aff-1e5722e20f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a1609-66c4-4d55-a8da-1bb335356d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54591ce7-7775-4632-baac-68c001b81ae7",
   "metadata": {},
   "source": [
    "1. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cddc73-5b83-4eda-b38b-782449909804",
   "metadata": {},
   "source": [
    "Let's first assess feature importance so we can figure out which features contribute the most to our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e331033-3110-4d6b-9971-51b8ee2f86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=2)\n",
    "\n",
    "# Stratified KFold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "cv_results = cross_validate(model, X, y, cv=skf, return_estimator=True)\n",
    "\n",
    "for estimator in cv_results['estimator']:\n",
    "    feature_importances.append(estimator.feature_importances_)\n",
    "\n",
    "# Calculate and display mean metrics\n",
    "mean_metrics = {key: np.mean(values) for key, values in cv_results.items() if key.startswith('test_')}\n",
    "print(\"Average Metrics across folds:\")\n",
    "for metric, value in mean_metrics.items():\n",
    "    print(f\"  {metric.replace('test_', '').capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Convert the list of feature importances to a DataFrame for easier interpretation\n",
    "feature_importances = np.array(feature_importances)\n",
    "\n",
    "# Average the feature importances across folds\n",
    "mean_importances = feature_importances.mean(axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding importances\n",
    "feature_names = X.columns  \n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': mean_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances in Random Forest (Average across folds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dde988-5db1-41f6-9fa7-948e925e79e4",
   "metadata": {},
   "source": [
    "Interpreting the feature importance graph, we see that the two most important features are NAT Source Port and Elapsed Time. Let's use random forest with a depth of 2 and a singular decision tree with a depth of 2 to test the accuracy of our model using these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed47db-757d-4aa2-8605-327d1766df37",
   "metadata": {},
   "source": [
    "We see that there is not much of a difference between the accuracy of the single decision tree and random forest. We can consider using one decision tree for this data since it computationally inexpensive, especially with a depth of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c8bfa-b0d1-499f-845d-1fb7e75037a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics and confusion matrices\n",
    "tree_metrics = []\n",
    "rf_metrics = []\n",
    "tree_conf_matrices = []\n",
    "rf_conf_matrices = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train and predict with Decision Tree\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    tree_preds = tree_model.predict(X_test)\n",
    "    tree_probs = tree_model.predict_proba(X_test)[:, 1] if len(tree_model.classes_) == 2 else tree_model.predict_proba(X_test)\n",
    "    tree_conf_matrices.append(confusion_matrix(y_test, tree_preds))\n",
    "\n",
    "    # Compute metrics for Decision Tree\n",
    "    tree_precision = precision_score(y_test, tree_preds, average='weighted')\n",
    "    tree_recall = recall_score(y_test, tree_preds, average='weighted')\n",
    "    tree_f1 = f1_score(y_test, tree_preds, average='weighted')\n",
    "    tree_auc = roc_auc_score(y_test, tree_probs, multi_class='ovr') if len(tree_model.classes_) > 2 else roc_auc_score(y_test, tree_probs)\n",
    "    tree_metrics.append((tree_precision, tree_recall, tree_f1, tree_auc))\n",
    "\n",
    "    # Train and predict with Random Forest\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_probs = rf_model.predict_proba(X_test)[:, 1] if len(rf_model.classes_) == 2 else rf_model.predict_proba(X_test)\n",
    "    rf_conf_matrices.append(confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "    # Compute metrics for Random Forest\n",
    "    rf_precision = precision_score(y_test, rf_preds, average='weighted')\n",
    "    rf_recall = recall_score(y_test, rf_preds, average='weighted')\n",
    "    rf_f1 = f1_score(y_test, rf_preds, average='weighted')\n",
    "    rf_auc = roc_auc_score(y_test, rf_probs, multi_class='ovr') if len(rf_model.classes_) > 2 else roc_auc_score(y_test, rf_probs)\n",
    "    rf_metrics.append((rf_precision, rf_recall, rf_f1, rf_auc))\n",
    "\n",
    "# Calculate averages across folds for both models\n",
    "tree_avg_metrics = pd.DataFrame(tree_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "rf_avg_metrics = pd.DataFrame(rf_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "\n",
    "# Print average metrics for both models\n",
    "print(\"Metrics for Decision Tree:\")\n",
    "print(f\"  Precision: {tree_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {tree_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {tree_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {tree_avg_metrics['AUC']:.4f}\")\n",
    "print()\n",
    "print(\"Metrics for Random Forest:\")\n",
    "print(f\"  Precision: {rf_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {rf_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {rf_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {rf_avg_metrics['AUC']:.4f}\")\n",
    "\n",
    "# Display confusion matrices for the last fold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ConfusionMatrixDisplay(tree_conf_matrices[-1], display_labels=tree_model.classes_).plot(ax=axes[0], cmap=\"Blues\")\n",
    "axes[0].set_title(\"Decision Tree Confusion Matrix (Last Fold)\")\n",
    "\n",
    "ConfusionMatrixDisplay(rf_conf_matrices[-1], display_labels=rf_model.classes_).plot(ax=axes[1], cmap=\"Blues\")\n",
    "axes[1].set_title(\"Random Forest Confusion Matrix (Last Fold)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441de81-8368-4e6b-8b04-5dfc673d143f",
   "metadata": {},
   "source": [
    "2. K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4cf55-94ec-4ad8-90f4-bbc010e025ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Range of k values to test\n",
    "k_range = range(1, 21)  \n",
    "\n",
    "# Stratified K-Fold Cross-Validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store the error rate for each k\n",
    "knn_error_rates = []\n",
    "\n",
    "# Stratified K-Fold Cross-Validation for each k value\n",
    "for k in k_range:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    fold_errors = [] \n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        knn_preds = knn_model.predict(X_test)\n",
    "\n",
    "        fold_errors.append(1 - accuracy_score(y_test, knn_preds))\n",
    "\n",
    "    # Average error rate for this value of k\n",
    "    knn_error_rates.append(np.mean(fold_errors))\n",
    "\n",
    "# Plot the elbow curve to find the best value for k\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, knn_error_rates, marker='o', color='r', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.title('KNN Elbow Plot (Error Rate vs. k)', fontsize=14)\n",
    "plt.xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "plt.ylabel('Average Error Rate', fontsize=12)\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"KNN_Elbow_Plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d24db8-a92d-42e5-81f5-fc6ea74415d0",
   "metadata": {},
   "source": [
    "The lowest error rate for k looks to be 3, although all the values for k seem to have low error rates. We will stick with k = 3 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd40829-2139-449f-af42-9b51536605db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KNN model with 3 neighbors\n",
    "k_neighbors = 3\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Lists to store metrics and confusion matrices\n",
    "knn_metrics = []\n",
    "knn_conf_matrices = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the KNN model and make predictions\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    knn_preds = knn_model.predict(X_test)\n",
    "    knn_probs = knn_model.predict_proba(X_test)[:, 1] if len(knn_model.classes_) == 2 else knn_model.predict_proba(X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    knn_precision = precision_score(y_test, knn_preds, average='weighted')\n",
    "    knn_recall = recall_score(y_test, knn_preds, average='weighted')\n",
    "    knn_f1 = f1_score(y_test, knn_preds, average='weighted')\n",
    "    knn_auc = roc_auc_score(y_test, knn_probs, multi_class='ovr') if len(knn_model.classes_) > 2 else roc_auc_score(y_test, knn_probs)\n",
    "    \n",
    "    knn_metrics.append((knn_precision, knn_recall, knn_f1, knn_auc))\n",
    "    knn_conf_matrices.append(confusion_matrix(y_test, knn_preds))\n",
    "\n",
    "# Calculate average metrics\n",
    "knn_avg_metrics = pd.DataFrame(knn_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Average Metrics for KNN (k={k_neighbors}):\")\n",
    "print(f\"  Precision: {knn_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {knn_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {knn_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {knn_avg_metrics['AUC']:.4f}\")\n",
    "\n",
    "# Display the confusion matrix for the last fold\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(knn_conf_matrices[-1], display_labels=knn_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(f\"KNN Confusion Matrix (Last Fold, k={k_neighbors})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e4381",
   "metadata": {},
   "source": [
    "3.  Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f'SVM accuracy score: {acc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c2366",
   "metadata": {},
   "source": [
    "Because we get a high accuracy, we can verify this by doing k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize SVM with probability=True for AUC computation\n",
    "svm_model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Lists to store metrics and confusion matrices\n",
    "svm_metrics = []\n",
    "svm_conf_matrices = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the SVM model and make predictions\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    svm_preds = svm_model.predict(X_test)\n",
    "    svm_probs = svm_model.predict_proba(X_test)  \n",
    "\n",
    "    # Compute metrics\n",
    "    svm_precision = precision_score(y_test, svm_preds, average='weighted')\n",
    "    svm_recall = recall_score(y_test, svm_preds, average='weighted')\n",
    "    svm_f1 = f1_score(y_test, svm_preds, average='weighted')\n",
    "    svm_auc = roc_auc_score(\n",
    "        LabelBinarizer().fit_transform(y_test), svm_probs, multi_class='ovr'\n",
    "    )\n",
    "\n",
    "    # Append metrics for this fold\n",
    "    svm_metrics.append((svm_precision, svm_recall, svm_f1, svm_auc))\n",
    "    svm_conf_matrices.append(confusion_matrix(y_test, svm_preds))\n",
    "\n",
    "# Calculate average metrics\n",
    "svm_avg_metrics = pd.DataFrame(svm_metrics, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]).mean()\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Metrics for SVM:\")\n",
    "print(f\"  Precision: {svm_avg_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {svm_avg_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {svm_avg_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  AUC: {svm_avg_metrics['AUC']:.4f}\")\n",
    "\n",
    "# Display the confusion matrix for the last fold\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(svm_conf_matrices[-1], display_labels=svm_model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(f\"SVM Confusion Matrix (Last Fold)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db013a",
   "metadata": {},
   "source": [
    "Using SVM and what we determined to be the two most important features, we can see that this version model gives up 79% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X2 = X[['NAT Source Port', 'Elapsed Time (sec)']] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "#print(X_train, y_train)\n",
    "\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f'SVM accuracy score: {acc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ff5d6",
   "metadata": {},
   "source": [
    "4. Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "score = linear_model.score(X_test, y_test)\n",
    "print(f'linear model score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d5b3a",
   "metadata": {},
   "source": [
    "We can use K-fold cross validation to analyze our linear model. Here we see it has a high value for R² showing it explains a good amount of the variance in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec97e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "linear_accuracies = []\n",
    "linear_conf_matrices = []\n",
    "linear_rmse = []\n",
    "linear_r2 = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    linear_preds = linear_model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, linear_preds))\n",
    "    r2 = r2_score(y_test, linear_preds)\n",
    "\n",
    "    linear_rmse.append(rmse)\n",
    "    linear_r2.append(r2)\n",
    "\n",
    "avg_rmse = np.mean(linear_rmse)\n",
    "avg_r2 = np.mean(linear_r2)\n",
    "\n",
    "print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "print(f\"Average R²: {avg_r2:.4f}\")\n",
    "\n",
    "# create a plot of the residuals vs predicted values\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(linear_preds, y_test - linear_preds, color='blue')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Reference line at y=0\n",
    "plt.title(\"Residuals vs. Predicted Values\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(X.columns)\n",
    "num_cols = 3\n",
    "num_rows = (num_features + num_cols - 1) // num_cols  # Calculate number of rows to fit all features\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for idx, feature in enumerate(X.columns):\n",
    "    \n",
    "    linear_model.fit(internet_data[[feature]], y)\n",
    "    y_pred = linear_model.predict(internet_data[[feature]])\n",
    "  \n",
    "    axes[idx].scatter(internet_data[feature], y, color='blue', label='Actual Data')\n",
    "    axes[idx].plot(internet_data[feature], y_pred, color='red', label='Regression Line')\n",
    "    \n",
    "    axes[idx].set_title(f\"Linear Regression: Action vs {feature}\")\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel(\"Action\")\n",
    "    axes[idx].legend()\n",
    "\n",
    "for idx in range(num_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5df47",
   "metadata": {},
   "source": [
    "5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train the Multinomial Logistic Regression model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_test_preds = model.predict(X_test_pca)\n",
    "y_test_probs = model.predict_proba(X_test_pca)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_test_preds, average='weighted')\n",
    "recall = recall_score(y_test, y_test_preds, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_preds, average='weighted')\n",
    "\n",
    "# AUC (for binary and multiclass)\n",
    "if len(set(y_test)) > 2:  # Multiclass case\n",
    "    auc = roc_auc_score(pd.get_dummies(y_test), y_test_probs, multi_class='ovr')\n",
    "else:  # Binary case\n",
    "    auc = roc_auc_score(y_test, y_test_probs[:, 1])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_preds)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Metrics for Logistic Regression with PCA:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "print(f\"  AUC: {auc:.4f}\")\n",
    "\n",
    "# PCA decision boundary plot\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, edgecolors='k', marker='o', cmap=plt.cm.Paired)\n",
    "plt.title('Multinomial Logistic Regression Decision Boundary with PCA')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Display Confusion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=model.classes_).plot(cmap=\"Blues\")\n",
    "plt.title(\"Logistic Regression with PCA Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160d2db-4574-4e15-8a9d-9139eca4e2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
